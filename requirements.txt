numpy
pandas
scikit-learn
scipy
h5py
Pillow
tifffile
opencv-python
scikit-image
matplotlib
seaborn
albumentations
tqdm
joblib
jupyterlab
ipykernel
tensorboard
rich
meteostat
rioxarray
# rasterio (try pip; if pip fails use conda install -c conda-forge rasterio)
rasterio
torch
torchvision
torchaudio
numpy
pandas
matplotlib
pillow
scikit-learn
scipy
tqdm
seaborn
rasterio
meteostat
opencv-python
torch
torchvision
torchaudio
numpy
pandas
matplotlib
pillow
scikit-learn
scipy
tqdm
seaborn
rasterio
meteostat
opencv-python
jupyter
notebook
ipykernel
watchdog
# fetch_gpt_chatbot.py
import os
import h5py
import torch
import numpy as np
import pandas as pd
import torch.nn.functional as F
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
from pathlib import Path
from pymongo import MongoClient
from openai import OpenAI
import shutil

from train_fusion import UNet, MAX_CHANNELS, IMG_SIZE, pad_channels, try_load_dem, compute_slope_aspect, synth_weather, synth_sensor, scalar_dict_to_channels

# ----------------------------- Paths
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
CHECKPOINT_PATH = r"D:\rockfall_ai\checkpoints\best_multimodal.pt"
TEST_DIR = Path(r"D:\rockfall_ai\data\drone_img\landslide4sense\TestData\img")
NEWDATA_DIR = Path(r"D:\rockfall_ai\data\drone_img\NewData\img")
SAVE_PRED_DIR = Path(r"D:\rockfall_ai\data\drone_img\NewData\pred_masks")
SAVE_OVERLAY_DIR = SAVE_PRED_DIR / "overlays"
SAVE_PRED_DIR.mkdir(exist_ok=True, parents=True)
SAVE_OVERLAY_DIR.mkdir(exist_ok=True, parents=True)
NEWDATA_DIR.mkdir(exist_ok=True, parents=True)

# ----------------------------- MongoDB
MONGO_URI = "mongodb+srv://rockfall_user:rock123@cluster0.2qjbtsd.mongodb.net/?retryWrites=true&w=majority"
client = MongoClient(MONGO_URI)
db = client["rockfall_ai"]
summary_collection = db["risk_summary"]
pixels_collection = db["risk_pixels"]
history_collection = db["user_history"]
risk_images_collection = db["risk_images"]

# ----------------------------- OpenAI
client_ai = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ----------------------------- Load model
model = UNet(in_ch=MAX_CHANNELS).to(DEVICE)
checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)
model.load_state_dict(checkpoint["model_state"])
model.eval()
print("‚úÖ Model loaded and ready for inference.")

# ----------------------------- DEM / slope / aspect
dem = try_load_dem(None)
slope, aspect = compute_slope_aspect(dem)
slope_t = torch.from_numpy(slope).unsqueeze(0)
aspect_t = torch.from_numpy(aspect).unsqueeze(0)

# ----------------------------- User history
def get_user_history(user_id):
    doc = history_collection.find_one({"user_id": user_id})
    return doc["history"] if doc else []

def save_user_history(user_id, history):
    history_collection.update_one(
        {"user_id": user_id},
        {"$set": {"history": history, "last_access": pd.Timestamp.utcnow()}},
        upsert=True
    )

# ----------------------------- Overlay
def save_overlay(x, y_pred=None, save_path=None, alpha=0.4):
    img = x[:3].permute(1,2,0).cpu().numpy()
    img = np.clip(img,0,1)
    plt.figure(figsize=(12,4))
    plt.subplot(1,3,1); plt.imshow(img); plt.title("RGB"); plt.axis("off")
    plt.subplot(1,3,2); plt.imshow(img)
    if y_pred is not None:
        plt.imshow(y_pred.squeeze().cpu().numpy()>0.5, cmap='Blues', alpha=alpha)
    plt.title("Pred Overlay"); plt.axis("off")
    plt.subplot(1,3,3); plt.imshow(img); plt.title("Final Overlay"); plt.axis("off")
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path)
    plt.close()

# ----------------------------- GPT query
def query_gpt(user_id, prompt):
    history = get_user_history(user_id)
    messages = [{"role": "system", "content": "You summarize landslide/rockfall risks and answer user queries interactively."}]
    messages.extend(history)
    messages.append({"role":"user","content":prompt})

    response = client_ai.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        max_tokens=300
    )
    reply = response.choices[0].message.content

    history.append({"role":"user","content":prompt})
    history.append({"role":"assistant","content":reply})
    save_user_history(user_id, history)
    return reply

# ----------------------------- Save CSVs & Mongo
def save_csvs_to_mongo(image_name, mask):
    if mask.dim() == 3 and mask.shape[0] == 1:
        mask = mask.squeeze(0)
    mask_flat = mask.numpy().flatten()
    h, w = mask.shape[-2:]
    pixels = [{"image": image_name, "x": i % w, "y": i // w, "risk_level": int(mask_flat[i])} for i in range(mask_flat.size)]
    df_pixels = pd.DataFrame(pixels)
    pixels_collection.delete_many({"image": image_name})
    pixels_collection.insert_many(pixels)
    pixel_csv_path = SAVE_PRED_DIR / f"{image_name}_pixels.csv"
    df_pixels.to_csv(pixel_csv_path, index=False)

    low = int(sum(mask_flat == 0))
    medium = int(sum(mask_flat == 1))
    high = int(sum(mask_flat > 1))
    total = int(len(mask_flat))
    norm_risk = float(((medium * 0.5) + high * 1.0) / total if total > 0 else 0)

    summary_doc = {
        "image": image_name,
        "low_count": low,
        "medium_count": medium,
        "high_count": high,
        "total_pixels": total,
        "normalized_risk": norm_risk
    }
    summary_collection.update_one({"image": image_name}, {"$set": summary_doc}, upsert=True)
    summary_csv_path = SAVE_PRED_DIR / f"{image_name}_summary.csv"
    pd.DataFrame([summary_doc]).to_csv(summary_csv_path, index=False)
    return summary_doc

# ----------------------------- Save overlay to MongoDB
def save_image_to_mongo(image_name, overlay_path):
    with open(overlay_path, "rb") as f:
        img_bytes = f.read()
    risk_images_collection.update_one(
        {"image": image_name},
        {"$set": {"overlay": img_bytes}},
        upsert=True
    )

# ----------------------------- Process new image
def process_new_image(image_name):
    test_path = TEST_DIR / f"{image_name}.h5"
    if not test_path.exists():
        print(f"‚ùå Image {image_name} not found in TestData folder.")
        return None

    # Copy to NewData
    new_path = NEWDATA_DIR / f"{image_name}.h5"
    if not new_path.exists():
        shutil.copy(test_path, new_path)
        print(f"‚úÖ {image_name} copied to NewData.")

    with h5py.File(new_path, "r") as f:
        img = np.array(f.get("img"), dtype=np.float32)/255.0
    x = torch.from_numpy(img).permute(2,0,1).float()
    x = F.interpolate(x.unsqueeze(0), size=(IMG_SIZE,IMG_SIZE), mode='bilinear', align_corners=False).squeeze(0)
    ws_ch = scalar_dict_to_channels({**synth_weather(), **synth_sensor()}, IMG_SIZE)
    x_full = torch.cat([x, slope_t, aspect_t, ws_ch], dim=0)
    x_full = pad_channels(x_full).unsqueeze(0).to(DEVICE)

    with torch.no_grad():
        logits = model(x_full)
        probs = torch.sigmoid(logits)
        pred_mask = (probs>0.5).float().cpu()

    overlay_path = SAVE_OVERLAY_DIR / f"overlay_{image_name}.png"
    save_overlay(x_full.squeeze(0), y_pred=probs, save_path=overlay_path)
    summary = save_csvs_to_mongo(image_name, pred_mask)
    save_image_to_mongo(image_name, overlay_path)
    return summary

# ----------------------------- Interactive chatbot loop
if __name__ == "__main__":
    user_id = "Sana"
    print("ü§ñ Rockfall GPT Chatbot. Type 'exit' to quit.\n")

    while True:
        msg = input("You: ").strip()
        if msg.lower() in ["exit", "quit"]:
            break

        # Handle image query
        if msg.startswith("image:"):
            img_name = msg.split("image:")[1].strip()
            
            # Ensure proper image naming
            if not img_name.startswith("image_"):
                img_name = "image_" + img_name

            # Check if summary already exists
            summary = summary_collection.find_one({"image": img_name})

            # Process image if not in MongoDB
            if not summary:
                processed = process_new_image(img_name)
                if not processed:
                    print(f"‚ùå Could not process {img_name}. Make sure it exists in TestData.")
                    continue
                summary = summary_collection.find_one({"image": img_name})

            # Build GPT prompt using actual summary data
            prompt = f"""
You are a geotechnical risk analyst.
Image: {img_name}
Low risk pixels: {summary['low_count']}
Medium risk pixels: {summary['medium_count']}
High risk pixels: {summary['high_count']}
Normalized risk score: {summary['normalized_risk']:.3f}

Provide a short, clear summary and advice for this image.
"""
        else:
            # Free-form user query
            prompt = msg

        # Get GPT reply
        reply = query_gpt(user_id, prompt)
        print(f"GPT: {reply}\n")



